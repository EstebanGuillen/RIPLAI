{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Arch:  DenseNet121\n",
      "Train for 511.0 steps, validate for 128.0 steps\n",
      "Epoch 1/25\n",
      "INFO:tensorflow:batch_all_reduce: 252 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 252 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "511/511 [==============================] - 171s 334ms/step - loss: 0.5636 - acc: 0.6973 - val_loss: 0.6055 - val_acc: 0.6521\n",
      "Epoch 2/25\n",
      "511/511 [==============================] - 30s 60ms/step - loss: 0.4907 - acc: 0.7591 - val_loss: 0.4531 - val_acc: 0.7844\n",
      "Epoch 3/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.4584 - acc: 0.7818 - val_loss: 0.4260 - val_acc: 0.8003\n",
      "Epoch 4/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.4336 - acc: 0.7972 - val_loss: 0.5235 - val_acc: 0.7551\n",
      "Epoch 5/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.4243 - acc: 0.8059 - val_loss: 0.5526 - val_acc: 0.7297\n",
      "Epoch 6/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.4156 - acc: 0.8041 - val_loss: 0.4533 - val_acc: 0.7979\n",
      "Epoch 7/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.4082 - acc: 0.8154 - val_loss: 0.5633 - val_acc: 0.7549\n",
      "Epoch 8/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3807 - acc: 0.8275 - val_loss: 0.4872 - val_acc: 0.7705\n",
      "Epoch 9/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3765 - acc: 0.8312 - val_loss: 0.4238 - val_acc: 0.8044\n",
      "Epoch 10/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3717 - acc: 0.8379 - val_loss: 0.4576 - val_acc: 0.8208\n",
      "Epoch 11/25\n",
      "511/511 [==============================] - 33s 64ms/step - loss: 0.3501 - acc: 0.8450 - val_loss: 0.4157 - val_acc: 0.8040\n",
      "Epoch 12/25\n",
      "511/511 [==============================] - 33s 64ms/step - loss: 0.3465 - acc: 0.8479 - val_loss: 0.6139 - val_acc: 0.7854\n",
      "Epoch 13/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3239 - acc: 0.8604 - val_loss: 0.4776 - val_acc: 0.7947\n",
      "Epoch 14/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3191 - acc: 0.8631 - val_loss: 0.3993 - val_acc: 0.8376\n",
      "Epoch 15/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3334 - acc: 0.8595 - val_loss: 0.4140 - val_acc: 0.8159\n",
      "Epoch 16/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3122 - acc: 0.8715 - val_loss: 0.4292 - val_acc: 0.8079\n",
      "Epoch 17/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3152 - acc: 0.8704 - val_loss: 0.6025 - val_acc: 0.7314\n",
      "Epoch 18/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.3128 - acc: 0.8628 - val_loss: 0.4315 - val_acc: 0.8149\n",
      "Epoch 19/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.2896 - acc: 0.8779 - val_loss: 0.4310 - val_acc: 0.8110\n",
      "Epoch 20/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.2728 - acc: 0.8885 - val_loss: 0.4521 - val_acc: 0.8079\n",
      "Epoch 21/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.2726 - acc: 0.8885 - val_loss: 0.4937 - val_acc: 0.7842\n",
      "Epoch 22/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.2615 - acc: 0.8946 - val_loss: 0.5861 - val_acc: 0.7922\n",
      "Epoch 23/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.2676 - acc: 0.8903 - val_loss: 0.4610 - val_acc: 0.8167\n",
      "Epoch 24/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.2477 - acc: 0.8999 - val_loss: 0.4710 - val_acc: 0.8210\n",
      "Epoch 25/25\n",
      "511/511 [==============================] - 30s 59ms/step - loss: 0.2492 - acc: 0.8998 - val_loss: 0.4667 - val_acc: 0.7939\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import tensorflow.keras.applications\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121,DenseNet169,  \\\n",
    "                                          DenseNet201,InceptionResNetV2,  \\\n",
    "                                          InceptionV3,MobileNet,MobileNetV2,  \\\n",
    "                                          NASNetLarge,NASNetMobile,ResNet101,  \\\n",
    "                                          ResNet101V2,ResNet152,ResNet152V2,  \\\n",
    "                                          ResNet50,ResNet50V2,VGG16,VGG19,Xception  \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "EPOCHS = 25\n",
    "lr = 3e-3\n",
    "CHANNELS =3\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
    "\n",
    "FILTER_SIZE = 3\n",
    "\n",
    "lrs = [1e-4, 3e-3, 1e-2]\n",
    "batch_sizes = [16,32,64]\n",
    "\n",
    "val_data_dir = '/opt/AIStorage/PLAYGROUND/images/512/validation'\n",
    "val_data_dir = pathlib.Path(val_data_dir)\n",
    "\n",
    "data_dir = '/opt/AIStorage/PLAYGROUND/images/512/train'\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "\n",
    "val_image_count = len(list(val_data_dir.glob('*/*.png')))\n",
    "\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
    "\n",
    "val_list_ds = tf.data.Dataset.list_files(str(val_data_dir/'*/*'))\n",
    "\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    label = -1\n",
    "    if parts[-2] == 'negative':\n",
    "        label = tf.constant([1.0, 0.0])\n",
    "    else:\n",
    "        label = tf.constant([0.0, 1.0])\n",
    "    return label\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_png(img, channels=CHANNELS)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "val_labeled_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "VAL_STEPS_PER_EPOCH = np.ceil(val_image_count/BATCH_SIZE)\n",
    "\n",
    "def prepare_for_training(ds, shuffle=True, cache=True, shuffle_buffer_size=11000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size, reshuffle_each_iteration=True)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "train_ds = prepare_for_training(labeled_ds)\n",
    "\n",
    "valid_ds = prepare_for_training(val_labeled_ds, shuffle=False)\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "with mirrored_strategy.scope():\n",
    "\n",
    "    '''architectures = [(\"DenseNet121\",DenseNet121(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"DenseNet169\",DenseNet169(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"DenseNet201\",DenseNet201(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"InceptionResNetV2\",InceptionResNetV2(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"MobileNet\",MobileNet(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"MobileNetV2\",MobileNetV2(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"ResNet101\",ResNet101(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"ResNet101V2\",ResNet101V2(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"ResNet152\",ResNet152(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"ResNet152V2\",ResNet152V2(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"ResNet50\",ResNet50(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"ResNet50V2\",ResNet50V2(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"VGG16\",VGG16(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"VGG19\",VGG19(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),\n",
    "                 (\"Xception\",Xception(input_shape=IMG_SHAPE,include_top=False,weights='imagenet'))] '''\n",
    "    \n",
    "    architectures = [(\"DenseNet121\",DenseNet121(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')),]\n",
    "    \n",
    "    for arch in architectures:\n",
    "    \n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_policy(policy)\n",
    "        print(\"\")\n",
    "        print(\"Arch: \",arch[0])\n",
    "    \n",
    "        initializer = tf.keras.initializers.he_normal()\n",
    "        steps = np.ceil(image_count / BATCH_SIZE) * EPOCHS\n",
    "\n",
    "        #optimizer = tf.keras.optimizers.SGD(lr=lr)\n",
    "    \n",
    "        #optimizer = tf.keras.optimizers.RMSprop(lr=lr)\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "        base_model = arch[1]\n",
    "    \n",
    "        #base_model.trainable = False\n",
    "        \n",
    "        for layer in base_model.layers:\n",
    "            if layer.name.endswith('bn'):\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "                \n",
    "        global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "        batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        drop_out_1 = tf.keras.layers.Dropout(0.30)\n",
    "        dense_layer_1 = tf.keras.layers.Dense(512,activation='relu', kernel_initializer=initializer)\n",
    "\n",
    "        batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        drop_out_2 = tf.keras.layers.Dropout(0.5)\n",
    "        prediction_layer = tf.keras.layers.Dense(2)\n",
    "    \n",
    "        batch_norm_3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "          base_model,\n",
    "          global_average_layer,\n",
    "      \n",
    "          batch_norm_1,\n",
    "          drop_out_1,\n",
    "          dense_layer_1,\n",
    "      \n",
    "          batch_norm_2,\n",
    "          drop_out_2,\n",
    "          prediction_layer,\n",
    "          batch_norm_3\n",
    "        ])\n",
    "\n",
    "    \n",
    "        model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['acc'])\n",
    "    \n",
    "        #model.summary()\n",
    "    \n",
    "        history = model.fit(train_ds,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=valid_ds, \n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks=[])\n",
    "        print(\"\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.1-conda",
   "language": "python",
   "name": "tf2.1-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
