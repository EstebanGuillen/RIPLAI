{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import tensorflow.keras.applications\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "import cv2\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121,DenseNet169,  \\\n",
    "                                          DenseNet201,InceptionResNetV2,  \\\n",
    "                                          InceptionV3,MobileNet,MobileNetV2,  \\\n",
    "                                          NASNetLarge,NASNetMobile,ResNet101,  \\\n",
    "                                          ResNet101V2,ResNet152,ResNet152V2,  \\\n",
    "                                          ResNet50,ResNet50V2,VGG16,VGG19,Xception \n",
    "\n",
    "from tensorflow.keras.metrics import Accuracy, Precision, Recall, \\\n",
    "                                     FalsePositives, FalsePositives, \\\n",
    "                                     TruePositives, TrueNegatives\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "IMG_HEIGHT = 512\n",
    "IMG_WIDTH = 512\n",
    "EPOCHS = 6\n",
    "\n",
    "CHANNELS =3\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
    "\n",
    "FILTER_SIZE = 3\n",
    "\n",
    "lrs = [('6e-4',6e-4), ('1e-3',1e-3), ('6e-3',6e-3)]\n",
    "batch_sizes = [('8',8),('16',16)]\n",
    "BATCH_SIZE = batch_sizes[0][1]\n",
    "optimizers = ['Adam', 'Nadam', 'SGD']\n",
    "opt = 'SGD'\n",
    "\n",
    "l = lrs[1]\n",
    "lr = l[1]\n",
    "\n",
    "mode = 'mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.experimental.set_visible_devices(gpus[1], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module for Image Save\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def save_grayscale(image, output_dir, output_name):\n",
    "    \"\"\"\n",
    "    Save a 3D Numpy array (H, W, 1) as an image.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Image to save\n",
    "        output_dir (str): Output directory\n",
    "        output_name (str): Output name\n",
    "    \"\"\"\n",
    "    Path.mkdir(Path(output_dir), parents=True, exist_ok=True)\n",
    "\n",
    "    cv2.imwrite(str(Path(output_dir) / output_name), image)\n",
    "\n",
    "\n",
    "def save_rgb(image, output_dir, output_name):\n",
    "    \"\"\"\n",
    "    Save a 3D Numpy array (H, W, 3) as an image.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Image to save\n",
    "        output_dir (str): Output directory\n",
    "        output_name (str): Output name\n",
    "    \"\"\"\n",
    "    Path.mkdir(Path(output_dir), parents=True, exist_ok=True)\n",
    "\n",
    "    cv2.imwrite(\n",
    "        str(Path(output_dir) / output_name), cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_display(array, num_rows=None, num_columns=None):\n",
    "    \"\"\"\n",
    "    Display a list of images as a grid.\n",
    "    Args:\n",
    "        array (numpy.ndarray): 4D Tensor (batch_size, height, width, channels)\n",
    "    Returns:\n",
    "        numpy.ndarray: 3D Tensor as concatenation of input images on a grid\n",
    "    \"\"\"\n",
    "    if num_rows is not None and num_columns is not None:\n",
    "        total_grid_size = num_rows * num_columns\n",
    "        if total_grid_size < len(array):\n",
    "            warnings.warn(\n",
    "                Warning(\n",
    "                    \"Given values for num_rows and num_columns doesn't allow to display \"\n",
    "                    \"all images. Values have been overrided to respect at least num_columns\"\n",
    "                )\n",
    "            )\n",
    "            num_rows = math.ceil(len(array) / num_columns)\n",
    "    elif num_rows is not None:\n",
    "        num_columns = math.ceil(len(array) / num_rows)\n",
    "    elif num_columns is not None:\n",
    "        num_rows = math.ceil(len(array) / num_columns)\n",
    "    else:\n",
    "        num_rows = math.ceil(math.sqrt(len(array)))\n",
    "        num_columns = math.ceil(math.sqrt(len(array)))\n",
    "\n",
    "    number_of_missing_elements = num_columns * num_rows - len(array)\n",
    "    # We fill the array with np.zeros elements to obtain a perfect square\n",
    "    array = np.append(\n",
    "        array,\n",
    "        np.zeros((number_of_missing_elements, *array[0].shape)).astype(array.dtype),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    grid = np.concatenate(\n",
    "        [\n",
    "            np.concatenate(\n",
    "                array[index * num_columns : (index + 1) * num_columns], axis=1\n",
    "            )\n",
    "            for index in range(num_rows)\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def filter_display(array, num_rows=None, num_columns=None):\n",
    "    \"\"\"\n",
    "    Display a list of filter outputs as a greyscale images grid.\n",
    "    Args:\n",
    "        array (numpy.ndarray): 4D Tensor (batch_size, height, width, channels)\n",
    "    Returns:\n",
    "        numpy.ndarray: 3D Tensor as concatenation of input images on a grid\n",
    "    \"\"\"\n",
    "    return grid_display(\n",
    "        np.concatenate(np.rollaxis(array, 3, 1), axis=0), num_rows, num_columns\n",
    "    )\n",
    "\n",
    "\n",
    "def image_to_uint_255(image):\n",
    "    \"\"\"\n",
    "    Convert float images to int 0-255 images.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image. Can be either [0, 255], [0, 1], [-1, 1]\n",
    "    Returns:\n",
    "        numpy.ndarray:\n",
    "    \"\"\"\n",
    "    if image.dtype == np.uint8:\n",
    "        return image\n",
    "\n",
    "    if image.min() < 0:\n",
    "        image = (image + 1.0) / 2.0\n",
    "\n",
    "    return (image * 255).astype(\"uint8\")\n",
    "\n",
    "\n",
    "def heatmap_display(\n",
    "    heatmap, original_image, colormap=cv2.COLORMAP_VIRIDIS, image_weight=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply a heatmap (as an np.ndarray) on top of an original image.\n",
    "    Args:\n",
    "        heatmap (numpy.ndarray): Array corresponding to the heatmap\n",
    "        original_image (numpy.ndarray): Image on which we apply the heatmap\n",
    "        colormap (int): OpenCV Colormap to use for heatmap visualization\n",
    "        image_weight (float): An optional `float` value in range [0,1] indicating the weight of\n",
    "            the input image to be overlaying the calculated attribution maps. Defaults to `0.7`\n",
    "    Returns:\n",
    "        np.ndarray: Original image with heatmap applied\n",
    "    \"\"\"\n",
    "    heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "    image = image_to_uint_255(original_image)\n",
    "\n",
    "    heatmap = (heatmap - np.min(heatmap)) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "    heatmap = cv2.applyColorMap(\n",
    "        cv2.cvtColor((heatmap * 255).astype(\"uint8\"), cv2.COLOR_GRAY2BGR), colormap\n",
    "    )\n",
    "\n",
    "    output = cv2.addWeighted(\n",
    "        cv2.cvtColor(image, cv2.COLOR_RGB2BGR), image_weight, heatmap, 1, 0\n",
    "    )\n",
    "\n",
    "    return cv2.cvtColor(output, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core Module for Grad CAM Algorithm\n",
    "\"\"\"\n",
    "\n",
    "class GradCAM:\n",
    "\n",
    "    \"\"\"\n",
    "    Perform Grad CAM algorithm for a given input\n",
    "    Paper: [Grad-CAM: Visual Explanations from Deep Networks\n",
    "            via Gradient-based Localization](https://arxiv.org/abs/1610.02391)\n",
    "    \"\"\"\n",
    "\n",
    "    def explain(\n",
    "        self,\n",
    "        validation_data,\n",
    "        model,\n",
    "        class_index,\n",
    "        layer_name=None,\n",
    "        colormap=cv2.COLORMAP_VIRIDIS,\n",
    "        image_weight=0.7,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute GradCAM for a specific class index.\n",
    "        Args:\n",
    "            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n",
    "                to perform the method on. Tuple containing (x, y).\n",
    "            model (tf.keras.Model): tf.keras model to inspect\n",
    "            class_index (int): Index of targeted class\n",
    "            layer_name (str): Targeted layer for GradCAM. If no layer is provided, it is\n",
    "                automatically infered from the model architecture.\n",
    "            colormap (int): OpenCV Colormap to use for heatmap visualization\n",
    "            image_weight (float): An optional `float` value in range [0,1] indicating the weight of\n",
    "                the input image to be overlaying the calculated attribution maps. Defaults to `0.7`.\n",
    "        Returns:\n",
    "            numpy.ndarray: Grid of all the GradCAM\n",
    "        \"\"\"\n",
    "        images, _ = validation_data\n",
    "\n",
    "        if layer_name is None:\n",
    "            layer_name = self.infer_grad_cam_target_layer(model)\n",
    "\n",
    "        outputs, guided_grads = GradCAM.get_gradients_and_filters(\n",
    "            model, images, layer_name, class_index\n",
    "        )\n",
    "\n",
    "        cams = GradCAM.generate_ponderated_output(outputs, guided_grads)\n",
    "\n",
    "        heatmaps = np.array(\n",
    "            [\n",
    "                # not showing the actual image if image_weight=0\n",
    "                heatmap_display(cam.numpy(), image, colormap, image_weight)\n",
    "                for cam, image in zip(cams, images)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        grid = grid_display(heatmaps)\n",
    "\n",
    "        return grid\n",
    "\n",
    "    @staticmethod\n",
    "    def infer_grad_cam_target_layer(model):\n",
    "        \"\"\"\n",
    "        Search for the last convolutional layer to perform Grad CAM, as stated\n",
    "        in the original paper.\n",
    "        Args:\n",
    "            model (tf.keras.Model): tf.keras model to inspect\n",
    "        Returns:\n",
    "            str: Name of the target layer\n",
    "        \"\"\"\n",
    "        for layer in reversed(model.layers):\n",
    "            # Select closest 4D layer to the end of the network.\n",
    "            if len(layer.output_shape) == 4:\n",
    "                print(layer.name)\n",
    "                return layer.name\n",
    "\n",
    "        raise ValueError(\n",
    "            \"Model does not seem to contain 4D layer. Grad CAM cannot be applied.\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def get_gradients_and_filters(model, images, layer_name, class_index):\n",
    "        \"\"\"\n",
    "        Generate guided gradients and convolutional outputs with an inference.\n",
    "        Args:\n",
    "            model (tf.keras.Model): tf.keras model to inspect\n",
    "            images (numpy.ndarray): 4D-Tensor with shape (batch_size, H, W, 3)\n",
    "            layer_name (str): Targeted layer for GradCAM\n",
    "            class_index (int): Index of targeted class\n",
    "        Returns:\n",
    "            Tuple[tf.Tensor, tf.Tensor]: (Target layer outputs, Guided gradients)\n",
    "        \"\"\"\n",
    "        grad_model = tf.keras.models.Model(\n",
    "            [model.inputs], [model.get_layer(layer_name).output, model.output]\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(images, tf.float32)\n",
    "            conv_outputs, predictions = grad_model(inputs)\n",
    "            loss = predictions[:, class_index]\n",
    "\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "        \n",
    "\n",
    "        guided_grads = (\n",
    "            tf.cast(conv_outputs > 0, \"float32\") * tf.cast(grads > 0, \"float32\") * tf.cast(grads,\"float32\")\n",
    "        )\n",
    "\n",
    "        return conv_outputs, guided_grads\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_ponderated_output(outputs, grads):\n",
    "        \"\"\"\n",
    "        Apply Grad CAM algorithm scheme.\n",
    "        Inputs are the convolutional outputs (shape WxHxN) and gradients (shape WxHxN).\n",
    "        From there:\n",
    "            - we compute the spatial average of the gradients\n",
    "            - we build a ponderated sum of the convolutional outputs based on those averaged weights\n",
    "        Args:\n",
    "            output (tf.Tensor): Target layer outputs, with shape (batch_size, Hl, Wl, Nf),\n",
    "                where Hl and Wl are the target layer output height and width, and Nf the\n",
    "                number of filters.\n",
    "            grads (tf.Tensor): Guided gradients with shape (batch_size, Hl, Wl, Nf)\n",
    "        Returns:\n",
    "            List[tf.Tensor]: List of ponderated output of shape (batch_size, Hl, Wl, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        maps = [\n",
    "            GradCAM.ponderate_output(output, grad)\n",
    "            for output, grad in zip(outputs, grads)\n",
    "        ]\n",
    "\n",
    "        return maps\n",
    "\n",
    "    @staticmethod\n",
    "    def ponderate_output(output, grad):\n",
    "        \"\"\"\n",
    "        Perform the ponderation of filters output with respect to average of gradients values.\n",
    "        Args:\n",
    "            output (tf.Tensor): Target layer outputs, with shape (Hl, Wl, Nf),\n",
    "                where Hl and Wl are the target layer output height and width, and Nf the\n",
    "                number of filters.\n",
    "            grads (tf.Tensor): Guided gradients with shape (Hl, Wl, Nf)\n",
    "        Returns:\n",
    "            tf.Tensor: Ponderated output of shape (Hl, Wl, 1)\n",
    "        \"\"\"\n",
    "        weights = tf.reduce_mean(grad, axis=(0, 1))\n",
    "\n",
    "        # Perform ponderated sum : w_i * output[:, :, i]\n",
    "        cam = tf.reduce_sum(tf.multiply(tf.cast(weights,\"float32\"), tf.cast(output,'float32') ), axis=-1)\n",
    "\n",
    "        return cam\n",
    "\n",
    "    def save(self, grid, output_dir, output_name):\n",
    "        \"\"\"\n",
    "        Save the output to a specific dir.\n",
    "        Args:\n",
    "            grid (numpy.ndarray): Grid of all the heatmaps\n",
    "            output_dir (str): Output directory path\n",
    "            output_name (str): Output name\n",
    "        \"\"\"\n",
    "        save_rgb(grid, output_dir, output_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>normal</th>\n",
       "      <th>fracture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.25.32275753493272066455728468222013315182.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.25.276430725003951681573364027711855344001.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.25.253422208237850690154351736536198463586.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.25.235981286495328419407334450332699270503.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.25.209179633594774313844201191440779232058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image  normal  fracture\n",
       "0   2.25.32275753493272066455728468222013315182.png       0         1\n",
       "1  2.25.276430725003951681573364027711855344001.png       0         1\n",
       "2  2.25.253422208237850690154351736536198463586.png       0         1\n",
       "3  2.25.235981286495328419407334450332699270503.png       1         0\n",
       "4  2.25.209179633594774313844201191440779232058.png       0         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./train-one-hot.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce training set for quick iteration debugging\n",
    "#train_df = train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>normal</th>\n",
       "      <th>fracture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.25.266368834198878584063538399027795661402.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.25.198053970490389086906016595802053064438.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.25.132649454315604163163469913960889989839.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.25.141160371661672481971139116295496247657.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.25.190934592832011590665778618155747001337.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.25.85588580190228693739225557679679405803.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.25.304602438475911337153274891496014274303.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.25.114370779292559265124294169713118718955.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.25.166101061863990858418630862457202935915.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.25.257743718283809201969571483810430772338.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.25.54495198169520147160859852325403544229.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.25.292727948301527179667218102523753467442.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.25.284265087269803950190076522602740207552.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.25.171022958276350267827675806411270754580.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.25.338202553134443978713868138049386002174.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.25.293803030075648154268183819514854257869.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.25.220685474744735840984002265562904007370.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.25.13247590035894092833791743994309521651.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.25.64867168400351643876312745265717638711.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.25.105645311273691361391096369628157687647.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.25.172063609931232675801637079854459318876.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.25.9373891275847801485413100904935355253.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.25.259313464244602885616264617136315063645.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.25.8076852146535239740873015592226370917.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.25.275669087006496045380470624184410646442.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  normal  fracture\n",
       "0   2.25.266368834198878584063538399027795661402.png       0         1\n",
       "1   2.25.198053970490389086906016595802053064438.png       0         1\n",
       "2   2.25.132649454315604163163469913960889989839.png       1         0\n",
       "3   2.25.141160371661672481971139116295496247657.png       0         1\n",
       "4   2.25.190934592832011590665778618155747001337.png       0         1\n",
       "5    2.25.85588580190228693739225557679679405803.png       0         1\n",
       "6   2.25.304602438475911337153274891496014274303.png       1         0\n",
       "7   2.25.114370779292559265124294169713118718955.png       1         0\n",
       "8   2.25.166101061863990858418630862457202935915.png       1         0\n",
       "9   2.25.257743718283809201969571483810430772338.png       0         1\n",
       "10   2.25.54495198169520147160859852325403544229.png       0         1\n",
       "11  2.25.292727948301527179667218102523753467442.png       1         0\n",
       "12  2.25.284265087269803950190076522602740207552.png       0         1\n",
       "13  2.25.171022958276350267827675806411270754580.png       0         1\n",
       "14  2.25.338202553134443978713868138049386002174.png       0         1\n",
       "15  2.25.293803030075648154268183819514854257869.png       0         1\n",
       "16  2.25.220685474744735840984002265562904007370.png       0         1\n",
       "17   2.25.13247590035894092833791743994309521651.png       1         0\n",
       "18   2.25.64867168400351643876312745265717638711.png       1         0\n",
       "19  2.25.105645311273691361391096369628157687647.png       0         1\n",
       "20  2.25.172063609931232675801637079854459318876.png       0         1\n",
       "21    2.25.9373891275847801485413100904935355253.png       0         1\n",
       "22  2.25.259313464244602885616264617136315063645.png       0         1\n",
       "23    2.25.8076852146535239740873015592226370917.png       0         1\n",
       "24  2.25.275669087006496045380470624184410646442.png       0         1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = pd.read_csv(\"./val-one-hot.csv\")\n",
    "valid_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>normal</th>\n",
       "      <th>fracture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>2.25.310656859401367861691413406710441338016.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>2.25.159014496226658261501698112987163403282.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>2.25.300379945108852291003294002596377791253.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>2.25.331675432825168281256094116304610068304.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>2.25.238514064404406518979636181721800247856.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>2.25.306015666503773354154907958287120514904.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>2.25.99487455393505948788675807627202878608.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2.25.145872723669425894281076127033757907791.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>2.25.217558881674041455047024948366123645839.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>2.25.220543078731579216886941763270870048757.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                image  normal  fracture\n",
       "782  2.25.310656859401367861691413406710441338016.png       1         0\n",
       "783  2.25.159014496226658261501698112987163403282.png       0         1\n",
       "784  2.25.300379945108852291003294002596377791253.png       1         0\n",
       "785  2.25.331675432825168281256094116304610068304.png       1         0\n",
       "786  2.25.238514064404406518979636181721800247856.png       1         0\n",
       "..                                                ...     ...       ...\n",
       "877  2.25.306015666503773354154907958287120514904.png       0         1\n",
       "878   2.25.99487455393505948788675807627202878608.png       0         1\n",
       "879  2.25.145872723669425894281076127033757907791.png       0         1\n",
       "880  2.25.217558881674041455047024948366123645839.png       0         1\n",
       "881  2.25.220543078731579216886941763270870048757.png       0         1\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>normal</th>\n",
       "      <th>fracture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.25.173789258248807928758296515316743888964.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.25.307941367272363205123396420445524060815.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.25.21224089373954987773412215178159360714.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.25.227746943974749430406884127570205991943.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.25.225671999229605271677288817920080550497.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image  normal  fracture\n",
       "0  2.25.173789258248807928758296515316743888964.png       0         1\n",
       "1  2.25.307941367272363205123396420445524060815.png       0         1\n",
       "2   2.25.21224089373954987773412215178159360714.png       1         0\n",
       "3  2.25.227746943974749430406884127570205991943.png       0         1\n",
       "4  2.25.225671999229605271677288817920080550497.png       1         0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"./test-one-hot.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['fracture'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture = ('DenseNet201', \n",
    "                #DenseNet201(input_shape=IMG_SHAPE,include_top=False,weights='imagenet'),\n",
    "                #'conv2_block1_1_conv',\n",
    "                #'conv5_block32_2_conv')\n",
    "\n",
    "architecture = ('ResNet152V2', \n",
    "                ResNet152V2(input_shape=IMG_SHAPE,include_top=False,weights='imagenet'),\n",
    "               'conv2_block1_1_conv',\n",
    "               'conv5_block3_3_conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['normal','fracture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 512, target_h = 512):\n",
    "       \n",
    "    print(\"getting train generator...\") \n",
    "    # normalize images\n",
    "    image_generator = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=180,\n",
    "        height_shift_range=[-10,10],\n",
    "        width_shift_range=[-10,10],\n",
    "        rescale=1./255)\n",
    "    \n",
    "    # flow from directory with specified batch size\n",
    "    # and target image size\n",
    "    generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=2000, batch_size=8, seed=1, target_w = 512, target_h = 512):\n",
    "    \n",
    "    print(\"getting test and valid generators...\")\n",
    "    \n",
    "\n",
    "    # use sample to fit mean and std for test set generator\n",
    "    image_generator = ImageDataGenerator(\n",
    "       rescale=1./255)\n",
    "    \n",
    "    \n",
    "\n",
    "    # get test generator\n",
    "    valid_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=valid_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "\n",
    "    test_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    return valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting train generator...\n",
      "Found 7063 validated image filenames.\n",
      "getting test and valid generators...\n",
      "Found 882 validated image filenames.\n",
      "Found 882 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = \"/opt/AIStorage/PLAYGROUND/images/512/filtered/all_images\"\n",
    "train_generator = get_train_generator(train_df, IMAGE_DIR, \"image\", labels, batch_size=BATCH_SIZE)\n",
    "valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"image\", labels, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = valid_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    base_model_x = None\n",
    "    model_x=None\n",
    "    \n",
    "    base_model_x = architecture[1]\n",
    "    \n",
    "    #base_model.trainable = False\n",
    "        \n",
    "    for layer in base_model_x.layers:\n",
    "        if layer.name.endswith('bn'):\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "                \n",
    "    x = base_model_x.output\n",
    "        \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Dense(512,activation='relu')(x)\n",
    "        \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    predictions = tf.keras.layers.Dense(2, activation='softmax', dtype=tf.float32)(x)\n",
    "        \n",
    "    model_x = Model(inputs=base_model_x.input,outputs=predictions)\n",
    "    \n",
    "    return model_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_examples = [ ('2.25.312225116666381863237801067474791146955.png',0),\n",
    "                   ('2.25.331215779585395792556605116028426521840.png',0),\n",
    "                   ('2.25.294885570448455301317418340729685335337.png',1),\n",
    "                   ('2.25.29996689003180237805319604864278729987.png',1),\n",
    "                   ('2.25.77133591367680957046388157253278169022.png',1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [architecture[2], architecture[3]]\n",
    "indexes = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveGradCAMCallback(Callback):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir=None,\n",
    "        layer_names=None,\n",
    "        examples=None,\n",
    "        indexes=None\n",
    "    ):\n",
    "        super(SaveGradCAMCallback,self).__init__()\n",
    "        self.output_dir = output_dir\n",
    "        self.layer_names = layer_names\n",
    "        self.examples = examples\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "       \n",
    "        \n",
    "        #self.model.save_weights(str(self.output_dir) + '/saved_models')\n",
    "        \n",
    "        #policy = mixed_precision.Policy('float32')\n",
    "        \n",
    "        #mixed_precision.set_policy(policy)\n",
    "        \n",
    "        \n",
    "        #saved_model = create_model()\n",
    "        \n",
    "        saved_model = self.model\n",
    "        \n",
    "        #saved_model.load_weights(str(self.output_dir) + '/saved_models').expect_partial()\n",
    "        \n",
    "        \n",
    "        \n",
    "        for example in self.examples:\n",
    "            for layer_name in self.layer_names:\n",
    "                for index in self.indexes:\n",
    "                    IMAGE_PATH = IMAGE_DIR + '/' + example[0]\n",
    "        \n",
    "                    img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(512, 512))\n",
    "                    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "                    img = img/255.0\n",
    "\n",
    "                    data = ([img], None)\n",
    "\n",
    "                    explainer = GradCAM()\n",
    "                    \n",
    "                    grid = explainer.explain(data, saved_model, class_index=index, image_weight=0.75, layer_name=layer_name)\n",
    "    \n",
    "                    file_id = example[0][:-4]\n",
    "                    save_name = file_id + '--' + str(epoch) + '--' + layer_name + '-- ' + str(index) + '.png'\n",
    "                    \n",
    "                    save_location = self.output_dir / file_id / layer_name / str(index)\n",
    "                    #explainer.save(grid, self.output_dir, save_name)\n",
    "                    explainer.save(grid, save_location, save_name)\n",
    "                    \n",
    "        \n",
    "        #policy = mixed_precision.Policy('mixed_float16')\n",
    "        #mixed_precision.set_policy(policy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        IMAGE_PATH = \\'/opt/AIStorage/PLAYGROUND/images/512/positive/2.25.181219916381488720424316063428743387152.png\\'\\n        img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(512, 512))\\n        img = tf.keras.preprocessing.image.img_to_array(img)\\n\\n        img = img/255.0\\n\\n\\n        data = ([img], None)\\n\\n        index = 0\\n        explainer = GradCAM()\\n        \\n        saved_model.trainable = True\\n        \\n        #layer =\\'conv2_block1_1_conv\\'\\n        layer =\\'conv5_block32_2_conv\\'\\n    \\n        grid = explainer.explain(data, saved_model, class_index=index, image_weight=0.5, layer_name=layer)\\n    \\n        explainer.save(grid, \".\", \"grad_cam10_16__2x0_5_adam.png\")\\n        \\n        policy = mixed_precision.Policy(\\'mixed_float16\\')\\n        mixed_precision.set_policy(policy)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        \n",
    "'''        IMAGE_PATH = '/opt/AIStorage/PLAYGROUND/images/512/positive/2.25.181219916381488720424316063428743387152.png'\n",
    "        img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(512, 512))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "        img = img/255.0\n",
    "\n",
    "\n",
    "        data = ([img], None)\n",
    "\n",
    "        index = 0\n",
    "        explainer = GradCAM()\n",
    "        \n",
    "        saved_model.trainable = True\n",
    "        \n",
    "        #layer ='conv2_block1_1_conv'\n",
    "        layer ='conv5_block32_2_conv'\n",
    "    \n",
    "        grid = explainer.explain(data, saved_model, class_index=index, image_weight=0.5, layer_name=layer)\n",
    "    \n",
    "        explainer.save(grid, \".\", \"grad_cam10_16__2x0_5_adam.png\")\n",
    "        \n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_policy(policy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arch:  DenseNet201\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 883 steps, validate for 111 steps\n",
      "Epoch 1/6\n",
      "883/883 [==============================] - 422s 478ms/step - loss: 0.8107 - acc: 0.6401 - val_loss: 1.1084 - val_acc: 0.6440\n",
      "Epoch 2/6\n",
      "883/883 [==============================] - 366s 415ms/step - loss: 0.6115 - acc: 0.7214 - val_loss: 0.4771 - val_acc: 0.8016\n",
      "Epoch 3/6\n",
      "520/883 [================>.............] - ETA: 2:26 - loss: 0.5278 - acc: 0.7730"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "STEPS_PER_EPOCH = len(train_generator) \n",
    "VAL_STEPS_PER_EPOCH = len(valid_generator) \n",
    "\n",
    "print(\"\")\n",
    "print(\"Arch: \", architecture[0])\n",
    "\n",
    "optimizer = None\n",
    "        \n",
    "if opt == 'Adam':\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "elif opt == 'Nadam':\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=lr)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=lr, momentum=0.9, decay=1e-6, nesterov=True)    \n",
    "    \n",
    "model = create_model()\n",
    "\n",
    "#model.summary()\n",
    "        \n",
    "        \n",
    "model.compile(optimizer=optimizer,loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),metrics=['acc'])\n",
    "\n",
    "run_name = architecture[0] + '_' + l[0] + '_' + opt + '_' + str(BATCH_SIZE) + '_' + mode\n",
    "run_dir = datetime.now().strftime(\"%Y%m%d-%H%M%S.%f\") + '_' + run_name\n",
    "output_dir = Path(\"./logs/grad_cam\") / run_dir\n",
    "\n",
    "Path.mkdir(Path(output_dir), parents=True, exist_ok=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=valid_generator, \n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks=[SaveGradCAMCallback(output_dir=output_dir,\n",
    "                                                   layer_names=layer_names,\n",
    "                                                   examples=valid_examples,\n",
    "                                                   indexes=indexes)])\n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_dir / 'history.npy', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the history object as a map\n",
    "#history=np.load('history.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the model: \", len(model.layers))\n",
    "\n",
    "# Fine-tune from about the middle layer onwards\n",
    "fine_tune_at = int(.5 * len(model.layers))\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in model.layers[:fine_tune_at]:\n",
    "    if layer.name.endswith('bn'):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt == 'Adam':\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr/6)\n",
    "elif opt == 'Nadam':\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=lr/6)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=lr/6, momentum=0.9, decay=1e-6, nesterov=True)   \n",
    "    \n",
    "model.compile(optimizer=optimizer,loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 8\n",
    "total_epochs =  EPOCHS + fine_tune_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fine = model.fit(train_generator,\n",
    "                    epochs=total_epochs,\n",
    "                    initial_epoch =  history.epoch[-1],\n",
    "                    validation_data=valid_generator, \n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks=[SaveGradCAMCallback(output_dir=output_dir,\n",
    "                                                   layer_names=layer_names,\n",
    "                                                   examples=valid_examples,\n",
    "                                                   indexes=indexes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_dir / 'history_fine.npy', history_fine.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['acc']\n",
    "val_acc += history_fine.history['val_acc']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([EPOCHS-1,EPOCHS-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([EPOCHS-1,EPOCHS-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt == 'Adam':\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr/10)\n",
    "elif opt == 'Nadam':\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=lr/10)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=lr/10, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "    \n",
    "model.compile(optimizer=optimizer,loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_epochs = 8\n",
    "total_epochs =  EPOCHS + fine_tune_epochs + unfreeze_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_unfreeze = model.fit(train_generator,\n",
    "                    epochs=total_epochs,\n",
    "                    initial_epoch =  history_fine.epoch[-1],\n",
    "                    validation_data=valid_generator, \n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks=[SaveGradCAMCallback(output_dir=output_dir,\n",
    "                                                   layer_names=layer_names,\n",
    "                                                   examples=valid_examples,\n",
    "                                                   indexes=indexes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_dir / 'history_unfreeze.npy', history_unfreeze.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_unfreeze.history['acc']\n",
    "val_acc += history_unfreeze.history['val_acc']\n",
    "\n",
    "loss += history_unfreeze.history['loss']\n",
    "val_loss += history_unfreeze.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([EPOCHS-1,EPOCHS-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.plot([(EPOCHS+fine_tune_epochs)-1,(EPOCHS+fine_tune_epochs)-1],\n",
    "          plt.ylim(), label='Start Full Unfreeze')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([EPOCHS-1,EPOCHS-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.plot([(EPOCHS+fine_tune_epochs)-1,(EPOCHS+fine_tune_epochs)-1],\n",
    "          plt.ylim(), label='Start Full Unfreeze')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_dir / 'metrics.npy', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_dir / 'preds_test.npy', preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./test-one-hot.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_dir / 'y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.1-conda",
   "language": "python",
   "name": "tf2.1-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
